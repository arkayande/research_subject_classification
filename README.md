# Document Classification using DistilBERT ðŸ“„ðŸ“š

This NLP project classifies research papers based on their titles and abstracts into different science domains using the DistilBERT transformer model.

## Highlights
- Preprocessed textual data (titles + abstracts)
- Fine-tuned DistilBERT on domain-labeled data
- Achieved 7% higher accuracy than previous SOTA baselines
- Compared performance with traditional ML models

## Tools & Libraries
Python, Huggingface Transformers, DistilBERT, Scikit-learn, Pandas, Matplotlib

## Outcome
Developed a high-accuracy research domain classifier with strong generalization capabilities.

ðŸ”— [Project Repository](https://github.com/arkayande/research_subject_classification)
Dataset Link - [Papers by Subject](https://www.kaggle.com/datasets/arplusman/papers-by-subject)

